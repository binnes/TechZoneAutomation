{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"TechZone Automation \u00b6 This repo is to document exploration of TechZone Automation / Cloud Native Toolkit. It is not end-user documentation, but an interim place to record learning, work arounds, best practices and other useful information about TechZone Automation","title":"Home"},{"location":"#techzone-automation","text":"This repo is to document exploration of TechZone Automation / Cloud Native Toolkit. It is not end-user documentation, but an interim place to record learning, work arounds, best practices and other useful information about TechZone Automation","title":"TechZone Automation"},{"location":"bom/","text":"Bill of Materials \u00b6 To deploy infrastructure or software using TechZone Automation you need to start by creating a Bill of Materials (BOM) or using a predefined BOM. The TechZone Accelerator Toolkit site is the starting place to see the modules catalog and list of pre-defined solutions. The BOM defines the modules you want to install. Available modules can be found in the Module Catalog Todo Why are some modules not in the catalog (e.g. k8s-ocp-cluster - does this mean they are obsolete or incomplete? Dependencies \u00b6 When you want to install a module there maybe some dependencies that module needs to allow it to be installed. For example, if you want to use GitOps (ArgoCD) to install an application then GitOps needs to be available and configured on the target cluster. Similarly if you want to install and configure GitOps then I need to have a cluster to install GitOps into. Every module in TechZone automation defines it's dependencies in a module.yaml file in the module's github repository. Thr module's repo is linked by clicking on the module name in the modules catalog Alias \u00b6 Sometimes a module dependency can be satisfied by multiple modules. In this case an alias can be used, where a module can register that it satisfies an alias. An example of this is the alias cluster . Any module that makes a Kubernetes cluster available to other modules will define the cluster alias. This then provides a generic way to specify a cluster dependency. So long as a module is included in a BOM that satisfies the cluster alias the dependency will be met. If you explore the cluster category in the modules catalog you will see a number of options for installing a Kubernetes cluster, including an option to simple provide the login to an existing cluster - all these modules will satisfy the cluster dependency. Optional \u00b6 Some dependencies can be specified as optional. This is where a module may be able to complete the installation without the optional dependency being satisfied in the BOM. An example of this is that modules using GitOps to perform an installation will specify the cluster as optional. This is because the module doesn't need to interact with the cluster to complete the installation. The module can simple create content in a GitOps repository to complete the installation on a cluster. Outstanding Questions \u00b6 Todo In the module.yaml what are the refs and interface properties used for? What does it mean when a dependency has multiple refs (e.g. artifactory )? How does the platform property work - any implications/restrictions or is this a testing statement or is there logic in the toolkit tool which checks all modules support the deployed platform? How are dependencies resolved? if a single module satisfies a dependency is it automatically selected? can a default module be specified if there are multiple modules that satisfy a dependency and one is not included in a BOM? is there a feature to allow an OR with dependencies (e.g. I need to have a MySQL or Postgres DB)? Verifying all dependencies are resolved \u00b6 Once a BOM has been completed you can validate that all dependencies have been satisfied or can be resolved by running the iascable tool. This will be covered in more detail in the deploy section. iascable build -i my_bom.yaml where my_bom.yaml is the file containing the BOM you want to verify The iascable build command will create a folder called output, if it doesn't already exist, then generate a folder within the output folder named using the name property in the metadata section of the BOM. Within this folder will be a file called bom.yaml which will be an expanded version of the original BOM (my_bom.yaml), pulling in all dependencies. If a dependency cannot be automatically resolved you will get an error detailing which dependencies cannot be resolved.","title":"Bill of Materials"},{"location":"bom/#bill-of-materials","text":"To deploy infrastructure or software using TechZone Automation you need to start by creating a Bill of Materials (BOM) or using a predefined BOM. The TechZone Accelerator Toolkit site is the starting place to see the modules catalog and list of pre-defined solutions. The BOM defines the modules you want to install. Available modules can be found in the Module Catalog Todo Why are some modules not in the catalog (e.g. k8s-ocp-cluster - does this mean they are obsolete or incomplete?","title":"Bill of Materials"},{"location":"bom/#dependencies","text":"When you want to install a module there maybe some dependencies that module needs to allow it to be installed. For example, if you want to use GitOps (ArgoCD) to install an application then GitOps needs to be available and configured on the target cluster. Similarly if you want to install and configure GitOps then I need to have a cluster to install GitOps into. Every module in TechZone automation defines it's dependencies in a module.yaml file in the module's github repository. Thr module's repo is linked by clicking on the module name in the modules catalog","title":"Dependencies"},{"location":"bom/#alias","text":"Sometimes a module dependency can be satisfied by multiple modules. In this case an alias can be used, where a module can register that it satisfies an alias. An example of this is the alias cluster . Any module that makes a Kubernetes cluster available to other modules will define the cluster alias. This then provides a generic way to specify a cluster dependency. So long as a module is included in a BOM that satisfies the cluster alias the dependency will be met. If you explore the cluster category in the modules catalog you will see a number of options for installing a Kubernetes cluster, including an option to simple provide the login to an existing cluster - all these modules will satisfy the cluster dependency.","title":"Alias"},{"location":"bom/#optional","text":"Some dependencies can be specified as optional. This is where a module may be able to complete the installation without the optional dependency being satisfied in the BOM. An example of this is that modules using GitOps to perform an installation will specify the cluster as optional. This is because the module doesn't need to interact with the cluster to complete the installation. The module can simple create content in a GitOps repository to complete the installation on a cluster.","title":"Optional"},{"location":"bom/#outstanding-questions","text":"Todo In the module.yaml what are the refs and interface properties used for? What does it mean when a dependency has multiple refs (e.g. artifactory )? How does the platform property work - any implications/restrictions or is this a testing statement or is there logic in the toolkit tool which checks all modules support the deployed platform? How are dependencies resolved? if a single module satisfies a dependency is it automatically selected? can a default module be specified if there are multiple modules that satisfy a dependency and one is not included in a BOM? is there a feature to allow an OR with dependencies (e.g. I need to have a MySQL or Postgres DB)?","title":"Outstanding Questions"},{"location":"bom/#verifying-all-dependencies-are-resolved","text":"Once a BOM has been completed you can validate that all dependencies have been satisfied or can be resolved by running the iascable tool. This will be covered in more detail in the deploy section. iascable build -i my_bom.yaml where my_bom.yaml is the file containing the BOM you want to verify The iascable build command will create a folder called output, if it doesn't already exist, then generate a folder within the output folder named using the name property in the metadata section of the BOM. Within this folder will be a file called bom.yaml which will be an expanded version of the original BOM (my_bom.yaml), pulling in all dependencies. If a dependency cannot be automatically resolved you will get an error detailing which dependencies cannot be resolved.","title":"Verifying all dependencies are resolved"},{"location":"deploy/","text":"Deploy Bill of Materials \u00b6 When you have your Bill of Materials (BOM) you can deploy it to create the desired environment. Todo what is the preferred/opinionated way to pass variables into the deploy process? in the BOM (is a BOM a template to apply to multiple environments, or an environment specific configuration?) a variables file - variables.yaml or directly modify terraform variable files environment variables how are sensitive credentials supposed to be managed? Variables \u00b6 When a BOM is deployed there are some variables (values) that need to be provided to configure each module so the desired state can be achieved. When a module is created, a set of input variables are defined with the option of providing a default value for an input variable. Some of the input variables may be generated by a dependent module. The module.yaml file for a module defines the required input variables and if the variable comes from a dependent module. It is also possible for a module to define optional input variables. As the modules are Terraform modules, there are also the Terraform variable definitions (usually in variables.tf and output.tf). The Terraform files have the variable definitions along with any default values. Todo What is the relationship between the input/output variables defined in the module.yaml file and the Terraform input/output variables defined in the Terraform (.tf) files? is there any validation between the .yaml BOM and .tf files? what is the purpose of the variable information in the modules.yaml file? is it used to generate the Terraform files in the output folder? should the Terraform files be regarded as the source of truth? You will need to provide some input variables for most modules. These allow you to customise the deployment without needing to alter the module code. It is possible to add variable values to the BOM yaml file, but Terraform also provides a number of ways to provide values to input variables: using the -var command line argument to pass values when invoking Terraform on the command line or from a script from a file containing the variable values as environment variables. The environment variable format Terraform uses is TF_VAR_ prepended to the input variable name If the launch.sh script is used to launch a tools container (Docker or Podman), then there is a specific process in place to pass environment variables using a credentials.properties file, which will set the environment variables within the tools container. The credentials file is also used to set the environment variables inside the multipass virtual machine. The credentials.properties file contains lines of the format: export TF_VAR_variable_name = \"variable_value\" This format ensure the environment variables will be correctly set when using multipass or using containers. Note The -var command line argument to the terraform command can only be used if you are not using the apply.sh script. See the deploy section for details of running a deployment Todo need a new version of iascable to be released to incorporate this pull request . Until then the apply.sh script will need to be manually modified after iascable build is run If variables are not passed into the deploy and no default value has been defined, then you will be prompted to provide the values at deploy time Todo How to manage Terraform state in a shared, team environment?","title":"Deploy"},{"location":"deploy/#deploy-bill-of-materials","text":"When you have your Bill of Materials (BOM) you can deploy it to create the desired environment. Todo what is the preferred/opinionated way to pass variables into the deploy process? in the BOM (is a BOM a template to apply to multiple environments, or an environment specific configuration?) a variables file - variables.yaml or directly modify terraform variable files environment variables how are sensitive credentials supposed to be managed?","title":"Deploy Bill of Materials"},{"location":"deploy/#variables","text":"When a BOM is deployed there are some variables (values) that need to be provided to configure each module so the desired state can be achieved. When a module is created, a set of input variables are defined with the option of providing a default value for an input variable. Some of the input variables may be generated by a dependent module. The module.yaml file for a module defines the required input variables and if the variable comes from a dependent module. It is also possible for a module to define optional input variables. As the modules are Terraform modules, there are also the Terraform variable definitions (usually in variables.tf and output.tf). The Terraform files have the variable definitions along with any default values. Todo What is the relationship between the input/output variables defined in the module.yaml file and the Terraform input/output variables defined in the Terraform (.tf) files? is there any validation between the .yaml BOM and .tf files? what is the purpose of the variable information in the modules.yaml file? is it used to generate the Terraform files in the output folder? should the Terraform files be regarded as the source of truth? You will need to provide some input variables for most modules. These allow you to customise the deployment without needing to alter the module code. It is possible to add variable values to the BOM yaml file, but Terraform also provides a number of ways to provide values to input variables: using the -var command line argument to pass values when invoking Terraform on the command line or from a script from a file containing the variable values as environment variables. The environment variable format Terraform uses is TF_VAR_ prepended to the input variable name If the launch.sh script is used to launch a tools container (Docker or Podman), then there is a specific process in place to pass environment variables using a credentials.properties file, which will set the environment variables within the tools container. The credentials file is also used to set the environment variables inside the multipass virtual machine. The credentials.properties file contains lines of the format: export TF_VAR_variable_name = \"variable_value\" This format ensure the environment variables will be correctly set when using multipass or using containers. Note The -var command line argument to the terraform command can only be used if you are not using the apply.sh script. See the deploy section for details of running a deployment Todo need a new version of iascable to be released to incorporate this pull request . Until then the apply.sh script will need to be manually modified after iascable build is run If variables are not passed into the deploy and no default value has been defined, then you will be prompted to provide the values at deploy time Todo How to manage Terraform state in a shared, team environment?","title":"Variables"},{"location":"plan/","text":"Planning \u00b6 The following investigation needs to be done and written up in preparation for creating user documentation. The work will be split into 2 sections: Consumption - how to use TechZone Automation to create an OpenShift Cluster on different clouds then install and configure a defined set of software Contribute - how to contribute modules to TechZone Automation to extend the catalog of modules available to install. This may include open source software, other commercial software offerings Consumption \u00b6 Before someone can contribute a module they need to understand how TechZone Automation works and how to use it to lay down a cluster and set of software. The investigations in this section should: Setup \u00b6 cover system setup to be able to use TechZone Automation on: MacOS (Intel) MacOS (Apple Silicon) Windows (10 and 11) Linux (Fedora) Linux (Ubuntu) In addition to the OS options there should be clear instructions to cover the following container tooling options: Docker Podman (if this is going to be a supported option going forward?) Multipass Building BOM \u00b6 There is an extensive catalog of modules so some explanation is needed to cover the tooling available and also explain how modules work to build an entire system (dependencies, variables, etc....) Installing an environment using a BOM \u00b6 How to take a BOM and run it to create an environment (this should work with all allowed OS and container tool combinations) Contribution \u00b6 Todo Complete this part of the plan","title":"Plan"},{"location":"plan/#planning","text":"The following investigation needs to be done and written up in preparation for creating user documentation. The work will be split into 2 sections: Consumption - how to use TechZone Automation to create an OpenShift Cluster on different clouds then install and configure a defined set of software Contribute - how to contribute modules to TechZone Automation to extend the catalog of modules available to install. This may include open source software, other commercial software offerings","title":"Planning"},{"location":"plan/#consumption","text":"Before someone can contribute a module they need to understand how TechZone Automation works and how to use it to lay down a cluster and set of software. The investigations in this section should:","title":"Consumption"},{"location":"plan/#setup","text":"cover system setup to be able to use TechZone Automation on: MacOS (Intel) MacOS (Apple Silicon) Windows (10 and 11) Linux (Fedora) Linux (Ubuntu) In addition to the OS options there should be clear instructions to cover the following container tooling options: Docker Podman (if this is going to be a supported option going forward?) Multipass","title":"Setup"},{"location":"plan/#building-bom","text":"There is an extensive catalog of modules so some explanation is needed to cover the tooling available and also explain how modules work to build an entire system (dependencies, variables, etc....)","title":"Building BOM"},{"location":"plan/#installing-an-environment-using-a-bom","text":"How to take a BOM and run it to create an environment (this should work with all allowed OS and container tool combinations)","title":"Installing an environment using a BOM"},{"location":"plan/#contribution","text":"Todo Complete this part of the plan","title":"Contribution"},{"location":"reference/","text":"Reference material \u00b6 Supported runtime tools for iascable Thomas's blog entry Cloud-Native Expertise Roadmap Modules catalog Ascent tool Operate - Cloud Native Toolkit","title":"Reference"},{"location":"reference/#reference-material","text":"Supported runtime tools for iascable Thomas's blog entry Cloud-Native Expertise Roadmap Modules catalog Ascent tool Operate - Cloud Native Toolkit","title":"Reference material"},{"location":"setup/","text":"Setup and first deploy (multipass) \u00b6 \u00b6 !!!Warning: Multipass networking doesn't work (no external connectivity, though name resolution works) with Cisco AnyConnect running! Turning off Cisco AnyConnect and the networking works - you cannot start Cisco AnyConnect while multipass is running or the network will be killed. Todo This section is under construction - need to split into sections covering install, creating BOM and then deploying BOM instead of single set of steps Create or change into the directory containing your BOM then run the following commands: install multipass : brew install --cask multipass 2. download the cloud init file : curl https://raw.githubusercontent.com/cloud-native-toolkit/sre-utilities/main/cloud-init/cli-tools.yaml --output cli-tools.yaml 3. launch multipass vm : multipass launch --name cli-tools --cloud-init ./cli-tools.yaml 4. mount current directory into VM : ` multipass mount $PWD cli-tools:/automation 5. enter vm : multipass shell cli-tools 6. install iascable : curl -sL https://raw.githubusercontent.com/cloud-native-toolkit/iascable/main/install.sh | sudo bash this is different to the command given in the docs (pipe into bash not sh) create BOM e.g. my-ibm-vpc-gitops.yaml apiVersion : cloudnativetoolkit.dev/v1alpha1 kind : BillOfMaterial metadata : name : my-ibm-vpc-gitops spec : modules : - name : ibm-vpc - name : ibm-vpc-subnets - name : ibm-vpc-gateways - name : ibm-ocp-vpc variables : - name : worker_count value : 1 - name : gitops-repo - name : argocd-bootstrap 8. run iascable build: iascable build -i oc-dev.yaml 9. run the terraform apply (optionally a variables.yaml file can be created - if not you will be prompted for required values) : cd output/my-ibm-vpc-gitops ./apply.sh 10. answer any prompts for missing variable values, check the steps listed and confirm the actions by responding yes 11. wait for terraform and gitops to complete the install Issues: guidance on variable values to be provided is needed certain modules fail (sealed-secrets-controller image fails to pull from docker.io - timeout) Setup and first deploy (podman) \u00b6 \u00b6 Create or change into the directory containing your BOM then run the following commands: install podman : brew install podman 2. install iascable if not already installed : curl -sL https://raw.githubusercontent.com/cloud-native-toolkit/iascable/main/install.sh | sudo sh 3. initialise podman : podman machine init 4. enable podman root : podman machine set --rootful 5. start podman machine : podman machine start 6. create BOM e.g. my-ibm-vpc-gitops.yaml apiVersion : cloudnativetoolkit.dev/v1alpha1 kind : BillOfMaterial metadata : name : my-ibm-vpc-gitops spec : modules : - name : ibm-vpc - name : ibm-vpc-subnets - name : ibm-vpc-gateways - name : ibm-ocp-vpc variables : - name : worker_count value : 1 - name : gitops-repo - name : argocd-bootstrap 7. run iascable build: iascable build -i my-ibm-vpc-gitops.yaml 8. launch the tools container : cd output ./launch.sh podman --pull 9. copy the mounted directory to a container directory (needed as podman has issues with symbolic links on a mounted directory) : cp -R * /workspaces 10. run the terraform apply (optionally a variables.yaml file can be created - if not you will be prompted for required values) : ```shell cd /workspaces/my-ibm-vpc-gitops ./apply.sh ``` answer any prompts for missing variable values, check the steps listed and confirm the actions by responding yes wait for terraform and gitops to complete the install Note the launch script will attach a podman volume for the workspace filesystem, which persists across multiple container runs, so you may need to clear the workspaces directory if you don't need the content from previous runs. Setup and first deploy (Docker) \u00b6 \u00b6 Docker desktop should be installed and be running. create BOM e.g. my-ibm-vpc-gitops.yaml apiVersion : cloudnativetoolkit.dev/v1alpha1 kind : BillOfMaterial metadata : name : my-ibm-vpc-gitops spec : modules : - name : ibm-vpc - name : ibm-vpc-subnets - name : ibm-vpc-gateways - name : ibm-ocp-vpc variables : - name : worker_count value : 1 - name : gitops-repo - name : argocd-bootstrap 2. run iascable build: iascable build -i my-ibm-vpc-gitops.yaml 3. launch the tools container : cd output ./launch.sh docker --pull 4. copy the mounted directory to a container directory : cp -R * /workspaces 5. run the terraform apply (optionally a variables.yaml file can be created - if not you will be prompted for required values) : cd /workspaces/my-ibm-vpc-gitops ./apply.sh 6. answer any prompts for missing variable values, check the steps listed and confirm the actions by responding yes 7. wait for terraform and gitops to complete the install","title":"Setup"},{"location":"setup/#setup-and-first-deploy-multipass","text":"!!!Warning: Multipass networking doesn't work (no external connectivity, though name resolution works) with Cisco AnyConnect running! Turning off Cisco AnyConnect and the networking works - you cannot start Cisco AnyConnect while multipass is running or the network will be killed. Todo This section is under construction - need to split into sections covering install, creating BOM and then deploying BOM instead of single set of steps Create or change into the directory containing your BOM then run the following commands: install multipass : brew install --cask multipass 2. download the cloud init file : curl https://raw.githubusercontent.com/cloud-native-toolkit/sre-utilities/main/cloud-init/cli-tools.yaml --output cli-tools.yaml 3. launch multipass vm : multipass launch --name cli-tools --cloud-init ./cli-tools.yaml 4. mount current directory into VM : ` multipass mount $PWD cli-tools:/automation 5. enter vm : multipass shell cli-tools 6. install iascable : curl -sL https://raw.githubusercontent.com/cloud-native-toolkit/iascable/main/install.sh | sudo bash this is different to the command given in the docs (pipe into bash not sh) create BOM e.g. my-ibm-vpc-gitops.yaml apiVersion : cloudnativetoolkit.dev/v1alpha1 kind : BillOfMaterial metadata : name : my-ibm-vpc-gitops spec : modules : - name : ibm-vpc - name : ibm-vpc-subnets - name : ibm-vpc-gateways - name : ibm-ocp-vpc variables : - name : worker_count value : 1 - name : gitops-repo - name : argocd-bootstrap 8. run iascable build: iascable build -i oc-dev.yaml 9. run the terraform apply (optionally a variables.yaml file can be created - if not you will be prompted for required values) : cd output/my-ibm-vpc-gitops ./apply.sh 10. answer any prompts for missing variable values, check the steps listed and confirm the actions by responding yes 11. wait for terraform and gitops to complete the install Issues: guidance on variable values to be provided is needed certain modules fail (sealed-secrets-controller image fails to pull from docker.io - timeout)","title":"Setup and first deploy (multipass)\u00b6"},{"location":"setup/#setup-and-first-deploy-podman","text":"Create or change into the directory containing your BOM then run the following commands: install podman : brew install podman 2. install iascable if not already installed : curl -sL https://raw.githubusercontent.com/cloud-native-toolkit/iascable/main/install.sh | sudo sh 3. initialise podman : podman machine init 4. enable podman root : podman machine set --rootful 5. start podman machine : podman machine start 6. create BOM e.g. my-ibm-vpc-gitops.yaml apiVersion : cloudnativetoolkit.dev/v1alpha1 kind : BillOfMaterial metadata : name : my-ibm-vpc-gitops spec : modules : - name : ibm-vpc - name : ibm-vpc-subnets - name : ibm-vpc-gateways - name : ibm-ocp-vpc variables : - name : worker_count value : 1 - name : gitops-repo - name : argocd-bootstrap 7. run iascable build: iascable build -i my-ibm-vpc-gitops.yaml 8. launch the tools container : cd output ./launch.sh podman --pull 9. copy the mounted directory to a container directory (needed as podman has issues with symbolic links on a mounted directory) : cp -R * /workspaces 10. run the terraform apply (optionally a variables.yaml file can be created - if not you will be prompted for required values) : ```shell cd /workspaces/my-ibm-vpc-gitops ./apply.sh ``` answer any prompts for missing variable values, check the steps listed and confirm the actions by responding yes wait for terraform and gitops to complete the install Note the launch script will attach a podman volume for the workspace filesystem, which persists across multiple container runs, so you may need to clear the workspaces directory if you don't need the content from previous runs.","title":"Setup and first deploy (podman)\u00b6"},{"location":"setup/#setup-and-first-deploy-docker","text":"Docker desktop should be installed and be running. create BOM e.g. my-ibm-vpc-gitops.yaml apiVersion : cloudnativetoolkit.dev/v1alpha1 kind : BillOfMaterial metadata : name : my-ibm-vpc-gitops spec : modules : - name : ibm-vpc - name : ibm-vpc-subnets - name : ibm-vpc-gateways - name : ibm-ocp-vpc variables : - name : worker_count value : 1 - name : gitops-repo - name : argocd-bootstrap 2. run iascable build: iascable build -i my-ibm-vpc-gitops.yaml 3. launch the tools container : cd output ./launch.sh docker --pull 4. copy the mounted directory to a container directory : cp -R * /workspaces 5. run the terraform apply (optionally a variables.yaml file can be created - if not you will be prompted for required values) : cd /workspaces/my-ibm-vpc-gitops ./apply.sh 6. answer any prompts for missing variable values, check the steps listed and confirm the actions by responding yes 7. wait for terraform and gitops to complete the install","title":"Setup and first deploy (Docker)\u00b6"}]}