{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"TechZone Accelerator Toolkit \u00b6 This repo is to document exploration of TechZone Accelerator / Cloud Native Toolkit. It is not end-user documentation, but an interim place to record learning, work arounds, best practices and other useful information about TechZone Accelerator Toolkit","title":"Home"},{"location":"#techzone-accelerator-toolkit","text":"This repo is to document exploration of TechZone Accelerator / Cloud Native Toolkit. It is not end-user documentation, but an interim place to record learning, work arounds, best practices and other useful information about TechZone Accelerator Toolkit","title":"TechZone Accelerator Toolkit"},{"location":"bom/","text":"Bill of Materials \u00b6 To deploy infrastructure or software using TechZone Accelerator you need to start by creating a Bill of Materials (BOM) or using a predefined BOM. The TechZone Accelerator Toolkit site is the starting place to see the modules catalog and list of pre-defined solutions. The BOM defines the modules you want to install. Available modules can be found in the Module Catalog See the operate documentation for additional details of the BOM content Todo Why are some modules not in the catalog (e.g. k8s-ocp-cluster - does this mean they are obsolete or incomplete? Dependencies \u00b6 When you want to install a module there maybe some dependencies that module needs to allow it to be installed. For example, if you want to use GitOps (ArgoCD) to install an application then GitOps needs to be available and configured on the target cluster. Similarly if you want to install and configure GitOps then I need to have a cluster to install GitOps into. Every module in then TechZone Accelerator Toolkit defines it's dependencies in a module.yaml file in the module's github repository. The module's repo is linked by clicking on the module name in the modules catalog Alias \u00b6 Sometimes a module dependency can be satisfied by multiple modules. In this case an alias can be used, where a module can register that it satisfies an alias. An example of this is the alias cluster . Any module that makes a Kubernetes cluster available to other modules will define the cluster alias. This then provides a generic way to specify a cluster dependency. So long as a module is included in a BOM that satisfies the cluster alias the dependency will be met. If you explore the cluster category in the modules catalog you will see a number of options for installing a Kubernetes cluster, including an option to simple provide the login to an existing cluster - all these modules will satisfy the cluster dependency. Optional \u00b6 Some dependencies can be specified as optional. This is where a module may be able to complete the installation without the optional dependency being satisfied in the BOM. An example of this is that modules using GitOps to perform an installation will specify the cluster as optional. This is because the module doesn't need to interact with the cluster to complete the installation. The module can simple create content in a GitOps repository to complete the installation on a cluster. Outstanding Questions \u00b6 Todo In the module.yaml what are the refs and interface properties used for? What does it mean when a dependency has multiple refs (e.g. artifactory )? How does the platform property work - any implications/restrictions or is this a testing statement or is there logic in the toolkit tool which checks all modules support the deployed platform? How are dependencies resolved? if a single module satisfies a dependency is it automatically selected? can a default module be specified if there are multiple modules that satisfy a dependency and one is not included in a BOM? is there a feature to allow an OR with dependencies (e.g. I need to have a MySQL or Postgres DB)? Verifying all dependencies are resolved \u00b6 Once a BOM has been completed you can validate that all dependencies have been satisfied or can be resolved by running the iascable tool. This will be covered in more detail in the deploy section. iascable build -i my_bom.yaml where my_bom.yaml is the file containing the BOM you want to verify The iascable build command will create a folder called output, if it doesn't already exist, then generate a folder within the output folder named using the name property in the metadata section of the BOM. Within this folder will be a file called bom.yaml which will be an expanded version of the original BOM (my_bom.yaml), pulling in all dependencies. If a dependency cannot be automatically resolved you will get an error detailing which dependencies cannot be resolved. Variables \u00b6 Todo Important flag in modules and BOM will cause TFVARS template to be generated","title":"Bill of Materials"},{"location":"bom/#bill-of-materials","text":"To deploy infrastructure or software using TechZone Accelerator you need to start by creating a Bill of Materials (BOM) or using a predefined BOM. The TechZone Accelerator Toolkit site is the starting place to see the modules catalog and list of pre-defined solutions. The BOM defines the modules you want to install. Available modules can be found in the Module Catalog See the operate documentation for additional details of the BOM content Todo Why are some modules not in the catalog (e.g. k8s-ocp-cluster - does this mean they are obsolete or incomplete?","title":"Bill of Materials"},{"location":"bom/#dependencies","text":"When you want to install a module there maybe some dependencies that module needs to allow it to be installed. For example, if you want to use GitOps (ArgoCD) to install an application then GitOps needs to be available and configured on the target cluster. Similarly if you want to install and configure GitOps then I need to have a cluster to install GitOps into. Every module in then TechZone Accelerator Toolkit defines it's dependencies in a module.yaml file in the module's github repository. The module's repo is linked by clicking on the module name in the modules catalog","title":"Dependencies"},{"location":"bom/#alias","text":"Sometimes a module dependency can be satisfied by multiple modules. In this case an alias can be used, where a module can register that it satisfies an alias. An example of this is the alias cluster . Any module that makes a Kubernetes cluster available to other modules will define the cluster alias. This then provides a generic way to specify a cluster dependency. So long as a module is included in a BOM that satisfies the cluster alias the dependency will be met. If you explore the cluster category in the modules catalog you will see a number of options for installing a Kubernetes cluster, including an option to simple provide the login to an existing cluster - all these modules will satisfy the cluster dependency.","title":"Alias"},{"location":"bom/#optional","text":"Some dependencies can be specified as optional. This is where a module may be able to complete the installation without the optional dependency being satisfied in the BOM. An example of this is that modules using GitOps to perform an installation will specify the cluster as optional. This is because the module doesn't need to interact with the cluster to complete the installation. The module can simple create content in a GitOps repository to complete the installation on a cluster.","title":"Optional"},{"location":"bom/#outstanding-questions","text":"Todo In the module.yaml what are the refs and interface properties used for? What does it mean when a dependency has multiple refs (e.g. artifactory )? How does the platform property work - any implications/restrictions or is this a testing statement or is there logic in the toolkit tool which checks all modules support the deployed platform? How are dependencies resolved? if a single module satisfies a dependency is it automatically selected? can a default module be specified if there are multiple modules that satisfy a dependency and one is not included in a BOM? is there a feature to allow an OR with dependencies (e.g. I need to have a MySQL or Postgres DB)?","title":"Outstanding Questions"},{"location":"bom/#verifying-all-dependencies-are-resolved","text":"Once a BOM has been completed you can validate that all dependencies have been satisfied or can be resolved by running the iascable tool. This will be covered in more detail in the deploy section. iascable build -i my_bom.yaml where my_bom.yaml is the file containing the BOM you want to verify The iascable build command will create a folder called output, if it doesn't already exist, then generate a folder within the output folder named using the name property in the metadata section of the BOM. Within this folder will be a file called bom.yaml which will be an expanded version of the original BOM (my_bom.yaml), pulling in all dependencies. If a dependency cannot be automatically resolved you will get an error detailing which dependencies cannot be resolved.","title":"Verifying all dependencies are resolved"},{"location":"bom/#variables","text":"Todo Important flag in modules and BOM will cause TFVARS template to be generated","title":"Variables"},{"location":"deploy/","text":"Deploy Bill of Materials \u00b6 When you have your Bill of Materials (BOM) you can deploy it to create the desired environment. Todo what is the preferred/opinionated way to pass variables into the deploy process? in the BOM (is a BOM a template to apply to multiple environments, or an environment specific configuration?) a variables file - variables.yaml or directly modify terraform variable files environment variables how are sensitive credentials supposed to be managed? Variables \u00b6 When a BOM is deployed there are some variables (values) that need to be provided to configure each module so the desired state can be achieved. When a module is created, a set of input variables are defined with the option of providing a default value for an input variable. Some of the input variables may be generated by a dependent module. The module.yaml file for a module defines the required input variables and if the variable comes from a dependent module. It is also possible for a module to define optional input variables. As the modules are Terraform modules, there are also the Terraform variable definitions (usually in variables.tf and output.tf). The Terraform files have the variable definitions along with any default values. Todo What is the relationship between the input/output variables defined in the module.yaml file and the Terraform input/output variables defined in the Terraform (.tf) files? is there any validation between the .yaml BOM and .tf files? what is the purpose of the variable information in the modules.yaml file? is it used to generate the Terraform files in the output folder? should the Terraform files be regarded as the source of truth? You will need to provide some input variables for most modules. These allow you to customise the deployment without needing to alter the module code. It is possible to add variable values to the BOM yaml file, but Terraform also provides a number of ways to provide values to input variables: using the -var command line argument to pass values when invoking Terraform on the command line or from a script from a file containing the variable values as environment variables. The environment variable format Terraform uses is TF_VAR_ prepended to the input variable name If the launch.sh script is used to launch a tools container (Docker or Podman), then there is a specific process in place to pass environment variables using a credentials.properties file, which will set the environment variables within the tools container. The credentials file is also used to set the environment variables inside the multipass virtual machine. The credentials.properties file contains lines of the format: export TF_VAR_variable_name = \"variable_value\" This format ensure the environment variables will be correctly set when using multipass or using containers. Note The -var command line argument to the terraform command can only be used if you are not using the apply.sh script. See the deploy section for details of running a deployment Todo need a new version of iascable to be released to incorporate this pull request . Until then the apply.sh script will need to be manually modified after iascable build is run line 37 of apply.sh script should be environment_variable=$(env | grep \"${variable_name}\" | sed -E 's/.*=(.*).*/\\1/g') If variables are not passed into the deploy and no default value has been defined, then you will be prompted to provide the values at deploy time Deploying the Bill of Materials \u00b6 Todo How to manage Terraform state in a shared, team environment?","title":"Deploy"},{"location":"deploy/#deploy-bill-of-materials","text":"When you have your Bill of Materials (BOM) you can deploy it to create the desired environment. Todo what is the preferred/opinionated way to pass variables into the deploy process? in the BOM (is a BOM a template to apply to multiple environments, or an environment specific configuration?) a variables file - variables.yaml or directly modify terraform variable files environment variables how are sensitive credentials supposed to be managed?","title":"Deploy Bill of Materials"},{"location":"deploy/#variables","text":"When a BOM is deployed there are some variables (values) that need to be provided to configure each module so the desired state can be achieved. When a module is created, a set of input variables are defined with the option of providing a default value for an input variable. Some of the input variables may be generated by a dependent module. The module.yaml file for a module defines the required input variables and if the variable comes from a dependent module. It is also possible for a module to define optional input variables. As the modules are Terraform modules, there are also the Terraform variable definitions (usually in variables.tf and output.tf). The Terraform files have the variable definitions along with any default values. Todo What is the relationship between the input/output variables defined in the module.yaml file and the Terraform input/output variables defined in the Terraform (.tf) files? is there any validation between the .yaml BOM and .tf files? what is the purpose of the variable information in the modules.yaml file? is it used to generate the Terraform files in the output folder? should the Terraform files be regarded as the source of truth? You will need to provide some input variables for most modules. These allow you to customise the deployment without needing to alter the module code. It is possible to add variable values to the BOM yaml file, but Terraform also provides a number of ways to provide values to input variables: using the -var command line argument to pass values when invoking Terraform on the command line or from a script from a file containing the variable values as environment variables. The environment variable format Terraform uses is TF_VAR_ prepended to the input variable name If the launch.sh script is used to launch a tools container (Docker or Podman), then there is a specific process in place to pass environment variables using a credentials.properties file, which will set the environment variables within the tools container. The credentials file is also used to set the environment variables inside the multipass virtual machine. The credentials.properties file contains lines of the format: export TF_VAR_variable_name = \"variable_value\" This format ensure the environment variables will be correctly set when using multipass or using containers. Note The -var command line argument to the terraform command can only be used if you are not using the apply.sh script. See the deploy section for details of running a deployment Todo need a new version of iascable to be released to incorporate this pull request . Until then the apply.sh script will need to be manually modified after iascable build is run line 37 of apply.sh script should be environment_variable=$(env | grep \"${variable_name}\" | sed -E 's/.*=(.*).*/\\1/g') If variables are not passed into the deploy and no default value has been defined, then you will be prompted to provide the values at deploy time","title":"Variables"},{"location":"deploy/#deploying-the-bill-of-materials","text":"Todo How to manage Terraform state in a shared, team environment?","title":"Deploying the Bill of Materials"},{"location":"helm/","text":"Helm \u00b6 Helm is the Kubernetes packaging solution. It packages kubernetes deployment artifacts into a single, parameterised bundle that can deliver consistent deployments. Testing \u00b6 The workflow around Helm doesn't appear very well established. There is a testing capability build into helm, using hooks , but this doesn't appear to be in mainstream use. There are a number of different opinions and strategies around CI/CD helm development workflows: Kubernetes Helm Charts Testing 13 Best Practices for using Helm - this is weaker on testing How To Continuously Test and Deploy Your Helm Charts on Kubernetes Clusters Using Kind Testing strategy of Helm Chart - discussion on helm testing in Apache discussion forum Pre-install Validation \u00b6 It is possible to add a schema to validate input variables before a Helm install is run. This ensures that all required values are provided and in the correct format. The values validation uses a JSON Schema There is also the helm lint command to verify the helm chart is OK and the --dry-run flag to generate the deployment artifacts, which could be used to test the correct generation with different input values.","title":"Helm"},{"location":"helm/#helm","text":"Helm is the Kubernetes packaging solution. It packages kubernetes deployment artifacts into a single, parameterised bundle that can deliver consistent deployments.","title":"Helm"},{"location":"helm/#testing","text":"The workflow around Helm doesn't appear very well established. There is a testing capability build into helm, using hooks , but this doesn't appear to be in mainstream use. There are a number of different opinions and strategies around CI/CD helm development workflows: Kubernetes Helm Charts Testing 13 Best Practices for using Helm - this is weaker on testing How To Continuously Test and Deploy Your Helm Charts on Kubernetes Clusters Using Kind Testing strategy of Helm Chart - discussion on helm testing in Apache discussion forum","title":"Testing"},{"location":"helm/#pre-install-validation","text":"It is possible to add a schema to validate input variables before a Helm install is run. This ensures that all required values are provided and in the correct format. The values validation uses a JSON Schema There is also the helm lint command to verify the helm chart is OK and the --dry-run flag to generate the deployment artifacts, which could be used to test the correct generation with different input values.","title":"Pre-install Validation"},{"location":"learningEnvironment/","text":"Learning Environment \u00b6 To create a zero-cost learning experience we want to use a local, low resource vanilla Kubernetes cluster using minikube multipass on mac Multipass environment on Mac \u00b6 Create the cloud-init based on the Toolkit init file, call it cli-tools-minikube.yaml : ## cloud-init to create a VM with the following: ## ## - terraform ## - terragrunt ## - git ## - jq ## - yq ## - oc ## - kubectl ## - helm ## - ibmcloud cli apt : sources : helm.list : source : \"deb https://baltocdn.com/helm/stable/debian/ all main\" key : | -----BEGIN PGP PUBLIC KEY BLOCK----- mQINBF6yP7IBEADWk4aijQ7Vhj7wn2oz+8asnfzsD0+257qjWy1m+cN4RP6T2NBG S2M5+vzbsKNmGAja8jOpo46pHo/SCdc8Bwv+QHH+JbuBbDNEHwIBGV5p+ZRETiHq l8UsyUAPCWinKR6evZrANCBEzXtOEVJ4thuPoBuZkteKNTdPlOg9MBqD5zz+4iQX 2CJJNW7+1sxAAVozHJxjJbu6c84yPvNFAiCAct+x5WJZFJWuO+l55vl6va8cV7tw DgHomk+1Q7w00Z0gh28Pe1yfvvw3N+pCSYn88mSgZtdP3wz3pABkMe4wMobNWuyX bIjGMuFDs7vGBY6UCL6alI/VC7rrSZqJZjntuoNI0Xlfc3BjUHWzinlbA7UFk5Lv qZO61V439Wm4x2n1V+4Kj/nPwtgBrNghaeDjxWLlgqaqynltSpXYnv2qGWYLRUb9 WFymbYCJ0piqRdNVNNI8Ht9nFaya6qjDcIxFwFMF9QcrECG1HCK1M5JjdJpzr6Jq Z27/2ZG7DhabArSR5aoyBqhCylJfXugneDhitmilJiQd5EzefjiDO29EuBSMwkAs +IKg9jxGyI47m3+ITWhMDWTFTYBF/O69iKXfFvf4zrbfGMcf2w8vIOEBU3eTSYoY RhHXROedwcYpaVGJmsaT38QTSMqWTn12zlvmW5f6mEI5LQq398gN9eiWDwARAQAB tERIZWxtIGhvc3RlZCBieSBCYWx0byAoUmVwb3NpdG9yeSBzaWduaW5nKSA8Z3Bn c2VjdXJpdHlAZ2V0YmFsdG8uY29tPokCVAQTAQoAPhYhBIG/gy4vGc0qoEcZWSlK xIJ8GhaKBQJesj+yAhsvBQkSzAMABQsJCAcCBhUKCQgLAgQWAgMBAh4BAheAAAoJ EClKxIJ8GhaKPHEP/RRzvYCetoLeIj5FtedbeumGcWaJj97L4R1j7iK0dc0uvg0T 5JeMDttAt69dFPHyB0kR1BLSwgJBhYCtvwalvD/g7DmL5l5HIM7o/VrkXDay1Pee wkCclA18y2wNM5EXKAuoFX5FMkRpTtSQhMMllbKsNNSvwvEZWvqMQlwJ/2HgNoVl 2NtfY65UXHvIV2nTTmCVDq4OYBlHoUX5rRE7fOgFZ+u6Su7yopTYy13yY8ZVDNf/ qNUWqA41gRYnwYtSq1DogHq1dcyr/SW/pFsn4n4LjG+38CIkSjFKOeusg2KPybZx l/z0/l0Yv4pTaa91rh1hGWqhvYDbLr2XqvI1wpcsIRPpU8lasycyQ8EeI4B5FVel ea2Z6rvGtMG92wVNCZ6YMYzpvRA9iRgve4J4ztlCwr0Tm78vY/vZfU5jkPW1VOXJ 6nW/RJuc2mecuj8YpJtioNVPbfxE/CjCCnGEnqn511ZYqKGd+BctqoFlWeSihHst tuSqJoqjOmt75MuN6zUJ0s3Ao+tzCmYkQzn2LUwnYisioyTW4gMtlh/wsU6Rmims s5doyG2Mcc0QfstXLMthVkrBpbW4XT+Q6aTGUMlMv1BhKycDUmewI2AMNth5Hood iEt18+X26+Q2exojaMHOCdkUJ+C44XPDy6EvG4RyO4bILHz5obD/9QZO/lzK =BFdd -----END PGP PUBLIC KEY BLOCK----- packages : - git - jq - helm - unzip - openvpn - ca-certificates - graphviz - ubuntu-desktop - xrdp - firefox groups : - docker snap : commands : - [ install , docker ] - [ install , kubectl , --classic ] runcmd : - adduser ubuntu docker - mkdir -p /run/tmp - export TERRAFORM_VERSION=1.2.4 && curl -Lso /run/tmp/terraform.zip https://releases.hashicorp.com/terraform/${TERRAFORM_VERSION}/terraform_${TERRAFORM_VERSION}_linux_$(if [ `uname -m` = arm64 -o `uname -m` = aarch64 ]; then echo \"arm64\"; else echo \"amd64\"; fi).zip && mkdir -p /run/tmp/terraform && cd /run/tmp/terraform && unzip /run/tmp/terraform.zip && sudo mv ./terraform /usr/local/bin && cd - && rm -rf /run/tmp/terraform && rm /run/tmp/terraform.zip - export TERRAGRUNT_VERSION=0.36.10 && curl -sLo /run/tmp/terragrunt https://github.com/gruntwork-io/terragrunt/releases/download/v${TERRAGRUNT_VERSION}/terragrunt_linux_$(if [ `uname -m` = arm64 -o `uname -m` = aarch64 ]; then echo \"arm64\"; else echo \"amd64\"; fi) && chmod +x /run/tmp/terragrunt && sudo mv /run/tmp/terragrunt /usr/local/bin/terragrunt - export YQ_VERSION=4.25.2 && curl -Lso /run/tmp/yq \"https://github.com/mikefarah/yq/releases/download/v${YQ_VERSION}/yq_linux_$(if [ `uname -m` = arm64 -o `uname -m` = aarch64 ]; then echo \"arm64\"; else echo \"amd64\"; fi)\" && chmod +x /run/tmp/yq && sudo mv /run/tmp/yq /usr/local/bin/yq - curl -Lo /run/tmp/kubectl \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/$(if [ `uname -m` = arm64 -o `uname -m` = aarch64 ]; then echo \"arm64\"; else echo \"amd64\"; fi)/kubectl\" && chmod +x /run/tmp/kubectl && sudo mv /run/tmp/kubectl /usr/local/bin - export OPENSHIFT_CLI_VERSION=4.10 && sudo curl -Lo /usr/local/oc-client.tar.gz https://mirror.openshift.com/pub/openshift-v4/$(if [ `uname -m` = arm64 -o `uname -m` = aarch64 ]; then echo \"arm64\"; else echo \"amd64\"; fi)/clients/ocp/stable-${OPENSHIFT_CLI_VERSION}/openshift-client-linux.tar.gz && sudo mkdir /usr/local/oc-client && cd /usr/local/oc-client && tar xzf /usr/local/oc-client.tar.gz && sudo mv ./oc /usr/local/bin && cd - && sudo rm -rf /usr/local/oc-client && sudo rm /usr/local/oc-client.tar.gz - curl -fsSL https://clis.cloud.ibm.com/install/linux | sh && ibmcloud plugin install container-service -f && ibmcloud plugin install container-registry -f && ibmcloud plugin install observe-service -f && ibmcloud plugin install vpc-infrastructure -f && ibmcloud config --check-version=false - curl -sL https://iascable.cloudnativetoolkit.dev/install.sh | sh - curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-$(if [ `uname -m` = arm64 -o `uname -m` = aarch64 ]; then echo \"arm64\"; else echo \"amd64\"; fi) - install minikube-linux-$(if [ `uname -m` = arm64 -o `uname -m` = aarch64 ]; then echo \"arm64\"; else echo \"amd64\"; fi) /usr/local/bin/minikube The follow the steps below: launch the Multipass environment: multipass launch --name cli-tools --cloud-init ./cli-tools-minikube.yaml --disk 40G --mem 4G --cpus 4 --timeout 480 mount the current directory into the multipass environment: multipass mount $PWD cli-tools:/automation enter the Multipass environment: multipass shell cli-tools create the kubernetes environment: minikube start --driver = docker --addons = dashboard,dns,ingress Podman on mac Podman on Mac \u00b6 start podman machine podman machine init --cpus = 4 --memory = 6096 -v $HOME : $HOME --rootful --now You need to know details of the podman network to get minikube working: podman network ls my output looks like this: NETWORK ID NAME DRIVER 2f259bab93aa podman bridge note the name of the network - podman then get details of the network with the podman network inspace <network name> command: podman network inspect podman which generated the following output: [ { \"name\": \"podman\", \"id\": \"2f259bab93aaaaa2542ba43ef33eb990d0999ee1b9924b557b7be53c0b7a1bb9\", \"driver\": \"bridge\", \"network_interface\": \"podman0\", \"created\": \"2022-10-19T16:57:22.965684025+01:00\", \"subnets\": [ { \"subnet\": \"10.88.0.0/16\", \"gateway\": \"10.88.0.1\" } ], \"ipv6_enabled\": false, \"internal\": false, \"dns_enabled\": false, \"ipam_options\": { \"driver\": \"host-local\" } } ] the gateway address is needed when starting minikube - 10.88.0.1 in the example shown above. install minikube Todo add instructions create the kubernetes environment: minikube start --driver = podman --apiserver-ips = 127 .0.0.1,10.88.0.1 --addons = dashboard,dns,ingress Note change 10.88.0.1 in the above command to the gateway address of your podman network - found in step 1 Once minikube is started you need to find what port the api is being served on. To do this enter command podman ps this will generate output similar to CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES d3a7f23e854b gcr.io/k8s-minikube/kicbase:v0.0.34 10 minutes ago Up 10 minutes ago 0.0.0.0:32865->22/tcp, 0.0.0.0:40515->2376/tcp, 0.0.0.0:35427->5000/tcp, 0.0.0.0:39641->8443/tcp, 0.0.0.0:33853->32443/tcp minikube look for the local port being mapped to container port 8443 : 0.0.0.0:39641->8443/tcp in the example above within podman containers the minikube cluster api can be accessed at https://10.88.0.1:39641 . Note down the port number on your system as you will need it in step 11. expose dashboard via the ingress: cat <<EOF | kubectl create -f - apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: dashboard-ingress namespace: kubernetes-dashboard spec: rules: - host: dashboard.info http: paths: - pathType: Prefix path: \"/\" backend: service: name: kubernetes-dashboard port: number: 80 EOF add name resolution for dashboard by adding 127.0.0.1 dashboard.info to /etc/hosts file. Choose your preferred editor, but you will need to edit the file using sudo sudo nano /etc/hosts add the line 127.0.0.1 dashboard.info to the bottom of the file, then Ctrl-o to write the file then Ctrl-x to quit the editor expose the ingress outside the minikube container minikube tunnel enter your password when prompted, then press and hold the Ctrl key then press z . The prompt should return then enter bg and press return. Your dashboard should now be available access the dashboard at http://dashboard.info create/navigate to a directory where you want to work using the cd command. This must be within your home directory for podman containers to be able to access the directory. This will be referred to as the project directory ( e.g. ~/project/minikube ) create your first bill of materials cat <<EOF > tutorial-bom.yaml apiVersion: cloud.ibm.com/v1alpha1 kind: BillOfMaterial metadata: name: minikube-test spec: modules: - name: cluster-login EOF build the bill of material using command Temporary workaround The cluster login module is not yet published, so need to add it manually so iascable can find it place the terraform-cluster-login.tgz file in the same folder as the tutorial-bom.yaml file then run the following command tar zxvf terraform-cluster-login.tgz instead of the iascable command below this box use this command iascable build -i tutorial-bom.yaml -c file: ${ PWD } /terraform-cluster-login/output/cluster-login/index.yaml then make the terraform-cluster-login.tgz file available within the tools container by moving it into the output directory mv terraform-cluster-login.tgz output DON\"T RUN THE COMMAND BELOW - THAT IS FOR WHEN THIS WORKAROUND CAN BE REMOVED move to step 11 iascable build -i tutorial-bom.yaml create the configuration needed to log into the cluster. Change the server_url to the minikube api server address you discovered in step 3 cat <<EOF > tutorial/variables.yaml variables: - name: server_url value: https://10.88.0.1:39641 - name: cluster_login_user value: minikube - name: cluster_ca_cert value: $(base64 -i ~/.minikube/ca.crt) - name: cluster_user_cert value: $(base64 -i ~/.minikube/profiles/minikube/client.crt) - name: cluster_user_key value: $(base64 -i ~/.minikube/profiles/minikube/client.key) EOF change to the output directory and launch into the cli-tools container cd output ./launch.sh podman Temporary workaround The cluster login module is not yet published, so need to add it manually into the correct place for it to be found when the Bill of Materials is applied sudo mkdir /automation cd /automation sudo tar zxvf /terraform/terraform-cluster-login.tgz cd /terraform apply the bill of materials cd tutorial ./apply.sh set the kube config file to be able to access minikube within the cli-tools container export KUBECONFIG = /terraform/tutorial/terraform/.tmp/.kube/config","title":"Learning Env"},{"location":"learningEnvironment/#learning-environment","text":"To create a zero-cost learning experience we want to use a local, low resource vanilla Kubernetes cluster using minikube multipass on mac","title":"Learning Environment"},{"location":"learningEnvironment/#multipass-environment-on-mac","text":"Create the cloud-init based on the Toolkit init file, call it cli-tools-minikube.yaml : ## cloud-init to create a VM with the following: ## ## - terraform ## - terragrunt ## - git ## - jq ## - yq ## - oc ## - kubectl ## - helm ## - ibmcloud cli apt : sources : helm.list : source : \"deb https://baltocdn.com/helm/stable/debian/ all main\" key : | -----BEGIN PGP PUBLIC KEY BLOCK----- mQINBF6yP7IBEADWk4aijQ7Vhj7wn2oz+8asnfzsD0+257qjWy1m+cN4RP6T2NBG S2M5+vzbsKNmGAja8jOpo46pHo/SCdc8Bwv+QHH+JbuBbDNEHwIBGV5p+ZRETiHq l8UsyUAPCWinKR6evZrANCBEzXtOEVJ4thuPoBuZkteKNTdPlOg9MBqD5zz+4iQX 2CJJNW7+1sxAAVozHJxjJbu6c84yPvNFAiCAct+x5WJZFJWuO+l55vl6va8cV7tw DgHomk+1Q7w00Z0gh28Pe1yfvvw3N+pCSYn88mSgZtdP3wz3pABkMe4wMobNWuyX bIjGMuFDs7vGBY6UCL6alI/VC7rrSZqJZjntuoNI0Xlfc3BjUHWzinlbA7UFk5Lv qZO61V439Wm4x2n1V+4Kj/nPwtgBrNghaeDjxWLlgqaqynltSpXYnv2qGWYLRUb9 WFymbYCJ0piqRdNVNNI8Ht9nFaya6qjDcIxFwFMF9QcrECG1HCK1M5JjdJpzr6Jq Z27/2ZG7DhabArSR5aoyBqhCylJfXugneDhitmilJiQd5EzefjiDO29EuBSMwkAs +IKg9jxGyI47m3+ITWhMDWTFTYBF/O69iKXfFvf4zrbfGMcf2w8vIOEBU3eTSYoY RhHXROedwcYpaVGJmsaT38QTSMqWTn12zlvmW5f6mEI5LQq398gN9eiWDwARAQAB tERIZWxtIGhvc3RlZCBieSBCYWx0byAoUmVwb3NpdG9yeSBzaWduaW5nKSA8Z3Bn c2VjdXJpdHlAZ2V0YmFsdG8uY29tPokCVAQTAQoAPhYhBIG/gy4vGc0qoEcZWSlK xIJ8GhaKBQJesj+yAhsvBQkSzAMABQsJCAcCBhUKCQgLAgQWAgMBAh4BAheAAAoJ EClKxIJ8GhaKPHEP/RRzvYCetoLeIj5FtedbeumGcWaJj97L4R1j7iK0dc0uvg0T 5JeMDttAt69dFPHyB0kR1BLSwgJBhYCtvwalvD/g7DmL5l5HIM7o/VrkXDay1Pee wkCclA18y2wNM5EXKAuoFX5FMkRpTtSQhMMllbKsNNSvwvEZWvqMQlwJ/2HgNoVl 2NtfY65UXHvIV2nTTmCVDq4OYBlHoUX5rRE7fOgFZ+u6Su7yopTYy13yY8ZVDNf/ qNUWqA41gRYnwYtSq1DogHq1dcyr/SW/pFsn4n4LjG+38CIkSjFKOeusg2KPybZx l/z0/l0Yv4pTaa91rh1hGWqhvYDbLr2XqvI1wpcsIRPpU8lasycyQ8EeI4B5FVel ea2Z6rvGtMG92wVNCZ6YMYzpvRA9iRgve4J4ztlCwr0Tm78vY/vZfU5jkPW1VOXJ 6nW/RJuc2mecuj8YpJtioNVPbfxE/CjCCnGEnqn511ZYqKGd+BctqoFlWeSihHst tuSqJoqjOmt75MuN6zUJ0s3Ao+tzCmYkQzn2LUwnYisioyTW4gMtlh/wsU6Rmims s5doyG2Mcc0QfstXLMthVkrBpbW4XT+Q6aTGUMlMv1BhKycDUmewI2AMNth5Hood iEt18+X26+Q2exojaMHOCdkUJ+C44XPDy6EvG4RyO4bILHz5obD/9QZO/lzK =BFdd -----END PGP PUBLIC KEY BLOCK----- packages : - git - jq - helm - unzip - openvpn - ca-certificates - graphviz - ubuntu-desktop - xrdp - firefox groups : - docker snap : commands : - [ install , docker ] - [ install , kubectl , --classic ] runcmd : - adduser ubuntu docker - mkdir -p /run/tmp - export TERRAFORM_VERSION=1.2.4 && curl -Lso /run/tmp/terraform.zip https://releases.hashicorp.com/terraform/${TERRAFORM_VERSION}/terraform_${TERRAFORM_VERSION}_linux_$(if [ `uname -m` = arm64 -o `uname -m` = aarch64 ]; then echo \"arm64\"; else echo \"amd64\"; fi).zip && mkdir -p /run/tmp/terraform && cd /run/tmp/terraform && unzip /run/tmp/terraform.zip && sudo mv ./terraform /usr/local/bin && cd - && rm -rf /run/tmp/terraform && rm /run/tmp/terraform.zip - export TERRAGRUNT_VERSION=0.36.10 && curl -sLo /run/tmp/terragrunt https://github.com/gruntwork-io/terragrunt/releases/download/v${TERRAGRUNT_VERSION}/terragrunt_linux_$(if [ `uname -m` = arm64 -o `uname -m` = aarch64 ]; then echo \"arm64\"; else echo \"amd64\"; fi) && chmod +x /run/tmp/terragrunt && sudo mv /run/tmp/terragrunt /usr/local/bin/terragrunt - export YQ_VERSION=4.25.2 && curl -Lso /run/tmp/yq \"https://github.com/mikefarah/yq/releases/download/v${YQ_VERSION}/yq_linux_$(if [ `uname -m` = arm64 -o `uname -m` = aarch64 ]; then echo \"arm64\"; else echo \"amd64\"; fi)\" && chmod +x /run/tmp/yq && sudo mv /run/tmp/yq /usr/local/bin/yq - curl -Lo /run/tmp/kubectl \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/$(if [ `uname -m` = arm64 -o `uname -m` = aarch64 ]; then echo \"arm64\"; else echo \"amd64\"; fi)/kubectl\" && chmod +x /run/tmp/kubectl && sudo mv /run/tmp/kubectl /usr/local/bin - export OPENSHIFT_CLI_VERSION=4.10 && sudo curl -Lo /usr/local/oc-client.tar.gz https://mirror.openshift.com/pub/openshift-v4/$(if [ `uname -m` = arm64 -o `uname -m` = aarch64 ]; then echo \"arm64\"; else echo \"amd64\"; fi)/clients/ocp/stable-${OPENSHIFT_CLI_VERSION}/openshift-client-linux.tar.gz && sudo mkdir /usr/local/oc-client && cd /usr/local/oc-client && tar xzf /usr/local/oc-client.tar.gz && sudo mv ./oc /usr/local/bin && cd - && sudo rm -rf /usr/local/oc-client && sudo rm /usr/local/oc-client.tar.gz - curl -fsSL https://clis.cloud.ibm.com/install/linux | sh && ibmcloud plugin install container-service -f && ibmcloud plugin install container-registry -f && ibmcloud plugin install observe-service -f && ibmcloud plugin install vpc-infrastructure -f && ibmcloud config --check-version=false - curl -sL https://iascable.cloudnativetoolkit.dev/install.sh | sh - curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-$(if [ `uname -m` = arm64 -o `uname -m` = aarch64 ]; then echo \"arm64\"; else echo \"amd64\"; fi) - install minikube-linux-$(if [ `uname -m` = arm64 -o `uname -m` = aarch64 ]; then echo \"arm64\"; else echo \"amd64\"; fi) /usr/local/bin/minikube The follow the steps below: launch the Multipass environment: multipass launch --name cli-tools --cloud-init ./cli-tools-minikube.yaml --disk 40G --mem 4G --cpus 4 --timeout 480 mount the current directory into the multipass environment: multipass mount $PWD cli-tools:/automation enter the Multipass environment: multipass shell cli-tools create the kubernetes environment: minikube start --driver = docker --addons = dashboard,dns,ingress Podman on mac","title":"Multipass environment on Mac"},{"location":"learningEnvironment/#podman-on-mac","text":"start podman machine podman machine init --cpus = 4 --memory = 6096 -v $HOME : $HOME --rootful --now You need to know details of the podman network to get minikube working: podman network ls my output looks like this: NETWORK ID NAME DRIVER 2f259bab93aa podman bridge note the name of the network - podman then get details of the network with the podman network inspace <network name> command: podman network inspect podman which generated the following output: [ { \"name\": \"podman\", \"id\": \"2f259bab93aaaaa2542ba43ef33eb990d0999ee1b9924b557b7be53c0b7a1bb9\", \"driver\": \"bridge\", \"network_interface\": \"podman0\", \"created\": \"2022-10-19T16:57:22.965684025+01:00\", \"subnets\": [ { \"subnet\": \"10.88.0.0/16\", \"gateway\": \"10.88.0.1\" } ], \"ipv6_enabled\": false, \"internal\": false, \"dns_enabled\": false, \"ipam_options\": { \"driver\": \"host-local\" } } ] the gateway address is needed when starting minikube - 10.88.0.1 in the example shown above. install minikube Todo add instructions create the kubernetes environment: minikube start --driver = podman --apiserver-ips = 127 .0.0.1,10.88.0.1 --addons = dashboard,dns,ingress Note change 10.88.0.1 in the above command to the gateway address of your podman network - found in step 1 Once minikube is started you need to find what port the api is being served on. To do this enter command podman ps this will generate output similar to CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES d3a7f23e854b gcr.io/k8s-minikube/kicbase:v0.0.34 10 minutes ago Up 10 minutes ago 0.0.0.0:32865->22/tcp, 0.0.0.0:40515->2376/tcp, 0.0.0.0:35427->5000/tcp, 0.0.0.0:39641->8443/tcp, 0.0.0.0:33853->32443/tcp minikube look for the local port being mapped to container port 8443 : 0.0.0.0:39641->8443/tcp in the example above within podman containers the minikube cluster api can be accessed at https://10.88.0.1:39641 . Note down the port number on your system as you will need it in step 11. expose dashboard via the ingress: cat <<EOF | kubectl create -f - apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: dashboard-ingress namespace: kubernetes-dashboard spec: rules: - host: dashboard.info http: paths: - pathType: Prefix path: \"/\" backend: service: name: kubernetes-dashboard port: number: 80 EOF add name resolution for dashboard by adding 127.0.0.1 dashboard.info to /etc/hosts file. Choose your preferred editor, but you will need to edit the file using sudo sudo nano /etc/hosts add the line 127.0.0.1 dashboard.info to the bottom of the file, then Ctrl-o to write the file then Ctrl-x to quit the editor expose the ingress outside the minikube container minikube tunnel enter your password when prompted, then press and hold the Ctrl key then press z . The prompt should return then enter bg and press return. Your dashboard should now be available access the dashboard at http://dashboard.info create/navigate to a directory where you want to work using the cd command. This must be within your home directory for podman containers to be able to access the directory. This will be referred to as the project directory ( e.g. ~/project/minikube ) create your first bill of materials cat <<EOF > tutorial-bom.yaml apiVersion: cloud.ibm.com/v1alpha1 kind: BillOfMaterial metadata: name: minikube-test spec: modules: - name: cluster-login EOF build the bill of material using command Temporary workaround The cluster login module is not yet published, so need to add it manually so iascable can find it place the terraform-cluster-login.tgz file in the same folder as the tutorial-bom.yaml file then run the following command tar zxvf terraform-cluster-login.tgz instead of the iascable command below this box use this command iascable build -i tutorial-bom.yaml -c file: ${ PWD } /terraform-cluster-login/output/cluster-login/index.yaml then make the terraform-cluster-login.tgz file available within the tools container by moving it into the output directory mv terraform-cluster-login.tgz output DON\"T RUN THE COMMAND BELOW - THAT IS FOR WHEN THIS WORKAROUND CAN BE REMOVED move to step 11 iascable build -i tutorial-bom.yaml create the configuration needed to log into the cluster. Change the server_url to the minikube api server address you discovered in step 3 cat <<EOF > tutorial/variables.yaml variables: - name: server_url value: https://10.88.0.1:39641 - name: cluster_login_user value: minikube - name: cluster_ca_cert value: $(base64 -i ~/.minikube/ca.crt) - name: cluster_user_cert value: $(base64 -i ~/.minikube/profiles/minikube/client.crt) - name: cluster_user_key value: $(base64 -i ~/.minikube/profiles/minikube/client.key) EOF change to the output directory and launch into the cli-tools container cd output ./launch.sh podman Temporary workaround The cluster login module is not yet published, so need to add it manually into the correct place for it to be found when the Bill of Materials is applied sudo mkdir /automation cd /automation sudo tar zxvf /terraform/terraform-cluster-login.tgz cd /terraform apply the bill of materials cd tutorial ./apply.sh set the kube config file to be able to access minikube within the cli-tools container export KUBECONFIG = /terraform/tutorial/terraform/.tmp/.kube/config","title":"Podman on Mac"},{"location":"module/","text":"Create or Update a module \u00b6 Todo Work in progress - complete this content Create module \u00b6 Generate module metadata \u00b6 https://github.com/cloud-native-toolkit/software-everywhere/blob/main/catalog.yaml Building Catalog \u00b6 Template v1 catalog","title":"Create/Update Module"},{"location":"module/#create-or-update-a-module","text":"Todo Work in progress - complete this content","title":"Create or Update a module"},{"location":"module/#create-module","text":"","title":"Create module"},{"location":"module/#generate-module-metadata","text":"https://github.com/cloud-native-toolkit/software-everywhere/blob/main/catalog.yaml","title":"Generate module metadata"},{"location":"module/#building-catalog","text":"Template v1 catalog","title":"Building Catalog"},{"location":"plan/","text":"Planning \u00b6 The following investigation needs to be done and written up in preparation for creating user documentation. The work will be split into 2 sections: Consumption - how to use TechZone Accelerator Toolkit to create an OpenShift Cluster on different clouds then install and configure a defined set of software Contribute - how to contribute modules to TechZone Accelerator Toolkit to extend the catalog of modules available to install. This may include open source software, other commercial software offerings Consumption \u00b6 Before someone can contribute a module they need to understand how TechZone Accelerator Toolkit works and how to use it to lay down a cluster and set of software. The investigations in this section should: Setup \u00b6 cover system setup to be able to use TechZone Accelerator Toolkit on: MacOS (Intel) MacOS (Apple Silicon) Windows (10 and 11) Linux (Fedora) Linux (Ubuntu) In addition to the OS options there should be clear instructions to cover the following container tooling options: Docker Podman (if this is going to be a supported option going forward?) Multipass Building BOM \u00b6 There is an extensive catalog of modules so some explanation is needed to cover the tooling available and also explain how modules work to build an entire system (dependencies, variables, etc....) Installing an environment using a BOM \u00b6 How to take a BOM and run it to create an environment (this should work with all allowed OS and container tool combinations) Contribution \u00b6 Todo Complete this part of the plan","title":"Plan"},{"location":"plan/#planning","text":"The following investigation needs to be done and written up in preparation for creating user documentation. The work will be split into 2 sections: Consumption - how to use TechZone Accelerator Toolkit to create an OpenShift Cluster on different clouds then install and configure a defined set of software Contribute - how to contribute modules to TechZone Accelerator Toolkit to extend the catalog of modules available to install. This may include open source software, other commercial software offerings","title":"Planning"},{"location":"plan/#consumption","text":"Before someone can contribute a module they need to understand how TechZone Accelerator Toolkit works and how to use it to lay down a cluster and set of software. The investigations in this section should:","title":"Consumption"},{"location":"plan/#setup","text":"cover system setup to be able to use TechZone Accelerator Toolkit on: MacOS (Intel) MacOS (Apple Silicon) Windows (10 and 11) Linux (Fedora) Linux (Ubuntu) In addition to the OS options there should be clear instructions to cover the following container tooling options: Docker Podman (if this is going to be a supported option going forward?) Multipass","title":"Setup"},{"location":"plan/#building-bom","text":"There is an extensive catalog of modules so some explanation is needed to cover the tooling available and also explain how modules work to build an entire system (dependencies, variables, etc....)","title":"Building BOM"},{"location":"plan/#installing-an-environment-using-a-bom","text":"How to take a BOM and run it to create an environment (this should work with all allowed OS and container tool combinations)","title":"Installing an environment using a BOM"},{"location":"plan/#contribution","text":"Todo Complete this part of the plan","title":"Contribution"},{"location":"reference/","text":"Reference material \u00b6 Supported runtime tools for iascable Thomas's blog entry Cloud-Native Expertise Roadmap Modules catalog Ascent tool Operate - Cloud Native Toolkit","title":"Reference"},{"location":"reference/#reference-material","text":"Supported runtime tools for iascable Thomas's blog entry Cloud-Native Expertise Roadmap Modules catalog Ascent tool Operate - Cloud Native Toolkit","title":"Reference material"},{"location":"setup/","text":"Setup and first deploy (multipass) \u00b6 \u00b6 !!!Warning: Multipass networking doesn't work (no external connectivity, though name resolution works) with Cisco AnyConnect running! Turning off Cisco AnyConnect and the networking works - you cannot start Cisco AnyConnect while multipass is running or the network will be killed. Todo This section is under construction - need to split into sections covering install, creating BOM and then deploying BOM instead of single set of steps Create or change into the directory containing your BOM then run the following commands: install multipass : brew install --cask multipass download the cloud init file : curl https://raw.githubusercontent.com/cloud-native-toolkit/sre-utilities/main/cloud-init/cli-tools.yaml --output cli-tools.yaml launch multipass vm : multipass launch --name cli-tools --cloud-init ./cli-tools.yaml mount current directory into VM : ` multipass mount $PWD cli-tools:/automation enter vm : multipass shell cli-tools install iascable : curl -sL https://raw.githubusercontent.com/cloud-native-toolkit/iascable/main/install.sh | sudo bash this is different to the command given in the docs (pipe into bash not sh) create BOM e.g. my-ibm-vpc-gitops.yaml apiVersion : cloudnativetoolkit.dev/v1alpha1 kind : BillOfMaterial metadata : name : my-ibm-vpc-gitops spec : modules : - name : ibm-vpc - name : ibm-vpc-subnets - name : ibm-vpc-gateways - name : ibm-ocp-vpc variables : - name : worker_count value : 1 - name : gitops-repo - name : argocd-bootstrap run iascable build: iascable build -i oc-dev.yaml run the terraform apply (optionally a variables.yaml file can be created - if not you will be prompted for required values) : cd output/my-ibm-vpc-gitops ./apply.sh answer any prompts for missing variable values, check the steps listed and confirm the actions by responding yes wait for terraform and gitops to complete the install Issues: guidance on variable values to be provided is needed certain modules fail (sealed-secrets-controller image fails to pull from docker.io - timeout) Setup and first deploy (podman) \u00b6 \u00b6 Create or change into the directory containing your BOM then run the following commands: install podman : brew install podman install iascable if not already installed : curl -sL https://raw.githubusercontent.com/cloud-native-toolkit/iascable/main/install.sh | sudo sh initialise podman : podman machine init --rootful you can also give the podman vm more resource if your machine has sufficient resource: podman machine init --cpus 4 --memory 8096 --rootful start podman machine : podman machine start create BOM e.g. my-ibm-vpc-gitops.yaml apiVersion : cloudnativetoolkit.dev/v1alpha1 kind : BillOfMaterial metadata : name : my-ibm-vpc-gitops spec : modules : - name : ibm-vpc - name : ibm-vpc-subnets - name : ibm-vpc-gateways - name : ibm-ocp-vpc variables : - name : worker_count value : 1 - name : gitops-repo - name : argocd-bootstrap run iascable build: iascable build -i my-ibm-vpc-gitops.yaml launch the tools container : cd output ./launch.sh podman --pull copy the mounted directory to a container directory (needed as podman has issues with symbolic links on a mounted directory) : cp -R * /workspaces run the terraform apply (optionally a variables.yaml file can be created - if not you will be prompted for required values) : cd /workspaces/my-ibm-vpc-gitops ./apply.sh answer any prompts for missing variable values, check the steps listed and confirm the actions by responding yes wait for terraform and gitops to complete the install Note the launch script will attach a podman volume for the workspace filesystem, which persists across multiple container runs, so you may need to clear the workspaces directory if you don't need the content from previous runs. Setup and first deploy (Docker) \u00b6 \u00b6 Docker desktop should be installed and be running. create BOM e.g. my-ibm-vpc-gitops.yaml apiVersion : cloudnativetoolkit.dev/v1alpha1 kind : BillOfMaterial metadata : name : my-ibm-vpc-gitops spec : modules : - name : ibm-vpc - name : ibm-vpc-subnets - name : ibm-vpc-gateways - name : ibm-ocp-vpc variables : - name : worker_count value : 1 - name : gitops-repo - name : argocd-bootstrap run iascable build: iascable build -i my-ibm-vpc-gitops.yaml launch the tools container : cd output ./launch.sh docker --pull copy the mounted directory to a container directory : cp -R * /workspaces run the terraform apply (optionally a variables.yaml file can be created - if not you will be prompted for required values) : cd /workspaces/my-ibm-vpc-gitops ./apply.sh answer any prompts for missing variable values, check the steps listed and confirm the actions by responding yes wait for terraform and gitops to complete the install","title":"Setup"},{"location":"setup/#setup-and-first-deploy-multipass","text":"!!!Warning: Multipass networking doesn't work (no external connectivity, though name resolution works) with Cisco AnyConnect running! Turning off Cisco AnyConnect and the networking works - you cannot start Cisco AnyConnect while multipass is running or the network will be killed. Todo This section is under construction - need to split into sections covering install, creating BOM and then deploying BOM instead of single set of steps Create or change into the directory containing your BOM then run the following commands: install multipass : brew install --cask multipass download the cloud init file : curl https://raw.githubusercontent.com/cloud-native-toolkit/sre-utilities/main/cloud-init/cli-tools.yaml --output cli-tools.yaml launch multipass vm : multipass launch --name cli-tools --cloud-init ./cli-tools.yaml mount current directory into VM : ` multipass mount $PWD cli-tools:/automation enter vm : multipass shell cli-tools install iascable : curl -sL https://raw.githubusercontent.com/cloud-native-toolkit/iascable/main/install.sh | sudo bash this is different to the command given in the docs (pipe into bash not sh) create BOM e.g. my-ibm-vpc-gitops.yaml apiVersion : cloudnativetoolkit.dev/v1alpha1 kind : BillOfMaterial metadata : name : my-ibm-vpc-gitops spec : modules : - name : ibm-vpc - name : ibm-vpc-subnets - name : ibm-vpc-gateways - name : ibm-ocp-vpc variables : - name : worker_count value : 1 - name : gitops-repo - name : argocd-bootstrap run iascable build: iascable build -i oc-dev.yaml run the terraform apply (optionally a variables.yaml file can be created - if not you will be prompted for required values) : cd output/my-ibm-vpc-gitops ./apply.sh answer any prompts for missing variable values, check the steps listed and confirm the actions by responding yes wait for terraform and gitops to complete the install Issues: guidance on variable values to be provided is needed certain modules fail (sealed-secrets-controller image fails to pull from docker.io - timeout)","title":"Setup and first deploy (multipass)\u00b6"},{"location":"setup/#setup-and-first-deploy-podman","text":"Create or change into the directory containing your BOM then run the following commands: install podman : brew install podman install iascable if not already installed : curl -sL https://raw.githubusercontent.com/cloud-native-toolkit/iascable/main/install.sh | sudo sh initialise podman : podman machine init --rootful you can also give the podman vm more resource if your machine has sufficient resource: podman machine init --cpus 4 --memory 8096 --rootful start podman machine : podman machine start create BOM e.g. my-ibm-vpc-gitops.yaml apiVersion : cloudnativetoolkit.dev/v1alpha1 kind : BillOfMaterial metadata : name : my-ibm-vpc-gitops spec : modules : - name : ibm-vpc - name : ibm-vpc-subnets - name : ibm-vpc-gateways - name : ibm-ocp-vpc variables : - name : worker_count value : 1 - name : gitops-repo - name : argocd-bootstrap run iascable build: iascable build -i my-ibm-vpc-gitops.yaml launch the tools container : cd output ./launch.sh podman --pull copy the mounted directory to a container directory (needed as podman has issues with symbolic links on a mounted directory) : cp -R * /workspaces run the terraform apply (optionally a variables.yaml file can be created - if not you will be prompted for required values) : cd /workspaces/my-ibm-vpc-gitops ./apply.sh answer any prompts for missing variable values, check the steps listed and confirm the actions by responding yes wait for terraform and gitops to complete the install Note the launch script will attach a podman volume for the workspace filesystem, which persists across multiple container runs, so you may need to clear the workspaces directory if you don't need the content from previous runs.","title":"Setup and first deploy (podman)\u00b6"},{"location":"setup/#setup-and-first-deploy-docker","text":"Docker desktop should be installed and be running. create BOM e.g. my-ibm-vpc-gitops.yaml apiVersion : cloudnativetoolkit.dev/v1alpha1 kind : BillOfMaterial metadata : name : my-ibm-vpc-gitops spec : modules : - name : ibm-vpc - name : ibm-vpc-subnets - name : ibm-vpc-gateways - name : ibm-ocp-vpc variables : - name : worker_count value : 1 - name : gitops-repo - name : argocd-bootstrap run iascable build: iascable build -i my-ibm-vpc-gitops.yaml launch the tools container : cd output ./launch.sh docker --pull copy the mounted directory to a container directory : cp -R * /workspaces run the terraform apply (optionally a variables.yaml file can be created - if not you will be prompted for required values) : cd /workspaces/my-ibm-vpc-gitops ./apply.sh answer any prompts for missing variable values, check the steps listed and confirm the actions by responding yes wait for terraform and gitops to complete the install","title":"Setup and first deploy (Docker)\u00b6"}]}